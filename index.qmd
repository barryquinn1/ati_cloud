---
title: "High performance cloud computing in finance"
subtitle: "AI and Trading (FIN7030)"
author: "Barry Quinn"
format: 
  revealjs:
    logo: img/qbslogo.png
    css: ["fonts.css", "default", "mycssblend.css"]
    lib_dir: libs
    countdown: 120000
    highlight-style: github
    highlight-lines: true
    incremental: true
    slide-number: true
    ratio: "16:9"
    before-init: "https://platform.twitter.com/widgets.js"
    seal: true
    include-in-header: "mathjax-equation-numbers.html"
---

```{r}
#| include: false


reticulate::use_condaenv("base")
library(htmltools)
options(htmltools.dir.version = FALSE)
library(knitr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse)
library(babynames)
library(fontawesome) 
library(DiagrammeR)
library(xaringanExtra)
library(timevis)
xaringanExtra::use_logo(image_url = "img/redlogo.png")
knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options) {
    if (before) {
      # record the current time before each chunk
      now <<- Sys.time()
    } else {
      # calculate the time difference after a chunk
      res <- difftime(Sys.time(), now)
      # return a character string to show the time
      paste("Time for this code chunk to run:", res)
    }
  }
}))

```

## Outline
- [What is cloud computing?]{.salt}
- [Introduction to the Q-RaP]{.fat}
- [Taxonomy of parallel computing]{.acid}
- [Performance computing in Python]{.heat}

## Introduction {.middle}
::: {.pull-left-2}
- This course will expose you to the flexibility of cloud computing
- We will learn how to build agile analytics by combining `r fa(name="r-project")` `r fa(name="plus")` `r fa(name="python")` 

- Financial institutions have a long history of heavily investing in technology for gaining market share and defending their current position
- The democratisation of computing processing power by cloud providers, has lead to an explosion in financial machine learning applications.
- Today technology that would have costed millions 20 years ago can be rented for thousand from these cloud providers
:::

::: {.pull-right-1}
>Meriwther spent $20 million on a state-of-the-art computer system and hired a crack team of financial engineers to run the show at LTCM..... 
`r tufte::quote_footer('Scott Patterson, The Quants, 2010')`
:::

## What is cloud computing ? {.middle}

- .heat[Cloud computing shares many of the similarities of electricity]

--

- .large[In the early days of electrical power, corporations and factories where powered by on-site small-scale power plants.] 
- .large[Today, large scale power plants with efficient transmission networks have powered modern industrial society, allowing electricity to be at everyone's disposal ar reasonable prices]

--

- .heat[Much like electricity the core concept of cloud computing is resource sharing]

## What is cloud computing ? {.top}

.fat[definitions:]

>A model for enabling ubiquitious, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or server provider interaction

`r tufte::quote_footer("National Institute of Standards and Technology(NIST) of the US Department of Commerce, 2019")`

--

>Simply put, cloud computing is the delivery of computing services – including servers, storage, databases, networking, software, analytics and intelligence – over the Internet (**"the cloud"**) to offer faster innovation, flexible resources and economies of scale. Typically, you only pay for cloud services you use, helping you lower your operating costs, run your infrastructure more efficiently and scale as your business needs change.

`r tufte::quote_footer("Beginners guide to cloud computing, Microsoft Azure, 2020")`

## Essential characteristics of cloud computing solutions {.top}

.heatinline[The follow distinguish cloud computing from other solutions, such as on-site high performance computing servers]

- .large.fancy[On-demand self-service] 
  - A consumer can provision computing capacity as needed without interaction with service provider
- .large.fancy[Broad network access]
  - Computing resources are available to the consumer through the network and can  be accessed from mobile phones, tablets, laptops, and workstations.
- .large.fancy[Resource pooling]
  - Resources are dynamically assigned to customers' needs
- .large.fancy[Rapid elasticity]
  - Capacities can be reconfigured automatically to scale rapidly in response to the changing demand
- .large.fancy[Measured service]
  - The resource utilisation is automatically controlled and optimised by the cloud systems.  The usage is monitored, measured, and reported

## Why high performance computing and why now? {.middle}
[A key reason is the regulatory response to the flash crash of 2010]{.salt}
> On the 6th May, 2010 at 2.45 p.m.(EST), the US stock market experienced a nearly 10% drop in the Dow Jones Industrial Average, only to recover most of ther loss a few minutes later

<iframe width="560" height="315" src="https://www.youtube.com/embed/_ZDEWVJan0s" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Flash crashed explained
<img src="img/nyt_flashcrash.png" height="70%" width="70%" class="center">

## Why high performance computing and why now? {.middle}

.saltinline[It took the regulatory agencies (SEC) five months to come up with an investigation report on the 2010 flash crash]

.heatinline[In the congressional hearings after the event, the SEC official give the primary reason for the long delay as the volumne of data to be investigated (~20 terabytes)]

.heatinline[HPC scientific computing systems such as the National Research Scientific Computing center(NERSC). routinely work with hundreds of terrabytes in minutes]

.heatinline[After the congressional hearing Lawrence Berkley National Laboratory (LNBL) established the Computational Intelligence and Forecasting Technologies (CIFT) project to promote HPC tools and techniques for analysing of financial times series data]

## Taxonomy of parallel computing {.middle}
- .large[Computer processors process instructions sequentially.
- Traditional computing problems are serial by design.
- The birth of multiprocessors (*multicore processors*) has innovated a new type of computing problem:
]
.acid[how to utilise the parallel structure]
- .large[Roughly speaking, multicore processors are equivalent to running multiple computers at a time]

## Taxonomy of parallel computing {.middle}
.pull-left.acid[Parallel computing problems in contrast to serial computing problems, refer to the type of problem that can be divided into small Jobs which can be processed simultaneously ]
.pull-right[

```{r}
#| echo: false
#| fig-cap: "**embarrassingly parallel computing**"
DiagrammeR::grViz("
digraph rmarkdown {
'Input Data'->{Job1,Job2,Job3}
{Job1,Job2,Job3}->'Results'
} 
"
,height = 200
)

```

```{r}
#| echo: false
#| fig-cap: "**nonembarrassingly parallel computing**"
DiagrammeR::grViz("
digraph rmarkdown {
'Input Data'->{Job1,Job2,Job3}
{Job1,Job2,Job3}->'Results'
Job1->Job2[dir='both',label='Information',style='dashed']
Job2->Job3[dir='both',label='Information',style='dashed']
{rank=same; Job1;Job2;Job3}
} 
"
,height = 200
)
```
]

## Taxonomy of parallel computing {.top}

.pull-left[

```{r}
#| echo: false
#| fig-cap: "Figure 1: embarrassingly parallel computing"
DiagrammeR::grViz("
digraph rmarkdown {
'Input Data'->{Job1,Job2,Job3}
{Job1,Job2,Job3}->'Results'
} 
"
,height = 200
)

```

```{r}
#| echo: false
#| fig-cap: "Figure 2: nonembarrassingly parallel computing"
DiagrammeR::grViz("
digraph rmarkdown {
'Input Data'->{Job1,Job2,Job3}
{Job1,Job2,Job3}->'Results'
Job1->Job2[dir='both',label='Information',style='dashed']
Job2->Job3[dir='both',label='Information',style='dashed']
{rank=same; Job1;Job2;Job3}
} 
"
,height = 200
)
```
]
.pull-right[
-  Based on the dependence structure of sub-Jobs problems parallel computing problems can be further classified into *embarrassingly parallel* and  *non embarrassingly parallel*
- If sub-Jobs are independent then we have an *embarrassingly parallel* problem
- Otherwise we have a  *non embarrassingly parallel* problem
- In figure 1 there is no communication between Jobs, while in figure 2 some communication between Jobs is required
]

## Taxonomy of parallel computing

- .large[Problems can also be classified by there nature:
- .heatinline[Data parallelism] focuses on distributing data across different processors
- .heatinline[Job parallelism] focuses on distributing execution processes (subtasks) to different processors]

## The serial-parallel scale {.middle,left}
.left-column[
```{r}
#| echo: false
DiagrammeR::grViz("
digraph  {
graph [label='Figure 1: Inherently serial']
'Input Data'->Job1->'Results'->Job2->'More Results'->Job3
} 
"
,height = 400
,options = "margin: auto !important;"
)
```
]

.right-column[

- Another important aspect of parallel computing is whether the parallel computing problem is *scalable*.
- An inherently serial problem cannot be parallelised, e.g. Figure 1
- Scalable problems have either a scalable problem size or scalable parallelism.
- Either the solution time reduces with the increasing of parallelism or the performance of the solution increases with the problem size.
- The elasticity of the computing architecture is the key to the success of the processing of scalable problems

]

## Example from the finance industry {.middle}
####.heat[Monte Carlo (MC) Options Pricing]
- Using arbitrage pricing theory, the option price is equal to the expected payoff $V$ discounted by a factor $D$ which can be evaluated via MC method.
The Monte Carlo estimator of option price is given by
$$C_0=D \times \frac{1}{N} \sum_{\omega \in sampleset}V(\omega)$$
where N is the number of sample paths (simulations)

1. The simulation of each price path is independent of other paths, therefore it is easy to parallel process the simulation of different paths on different computing nodes
2. It is a task-parallel in the sense that simulations of each path is a small task.  However it is not data-parallel since there is no data set to be distributed to different computing nodes.
3. The scalable benefit is improved price accuracy as $N$ increases, the error reduction being approximately $1/\sqrt{N}$

## Example from the finance industry {.middle}
#####.heat[Backtesting Investment Strategies]

.pull-left[
#### Task-parallel backtesting
- Suppose you need to backtest a basket of different investment strategies to identify the optimal strategies
- The process of each investment strategy is independent of each other and can be run simultaneously.
- Distributing the processing of different strategies to different computing nodes is an *embarrassingly parallel* and *task parallel* implementation.
]
.pull-right[
#### Data-parallel backtesting
- Backtesting on one strategy can also be implemented as *data parallel*
- By generating subsamples from test data set(e.g. by bootstrapping), strategy can be processed on different subsamples simultaneously
- The results on different subsamples is then aggregated to generate the performance and risk report of the strategy
]

## R Python Interface <img src="img/reticulated_python.png" width="10%" style="float:right"/>the `reticulate` package {.middle}

- In your `Rmd` file, insert a `R code chunk` by clicking Insert and selecting R 
- `r fa('windows')` **Ctrl+Shift+i**
- `r fa('apple')` **Cmd+Shift+i**

```{r}
#| label: python+r set-up
library(reticulate)
```
- Can you insert a `python` code chunk and tell it to print "Hello World"?
--

```{python}
print("Hello World")
```

## Improving computational performance {.middle}

- There is a commonly held misconception that both Python and R are relatively slow programming languages and not appropriate to implement computationally demand tasks in finance.
- This criticism stems from the fact that most financial algorithms require loops and loops are slow on interpreted languages such as R and Python.
- Traditionally C or C++, which are compiled programming languages, have been used to speed up loops. 

## Performance Python {.middle}
```{r}
#| echo: false
xaringanExtra::use_tachyons()
```
- We will consider a number of approaches to speed up typical tasks and algorithms often encountered in finance including:
.bg-gold[
1. **Vectorisation**<br>Making use of Python's vectorisation which can effectively remove a loop structure of a problem
2. **Dynamic Compiling**<br> Using the `Numba` package allows dynamic compilations of pure Python code using [LLVM technology](https://llvm.org/)
3. **Static Compiling**:<br> `Cython` is not only a Python package but a hybrid language that combines Python and C.
4. **Multiprocessing**:<br>Finally we can excute the code in parallel using the `multiprocessing`Python package
]

## Loops in Python {.middle}

.panelset[
.panel[.panel-name[Explanation]
- We will start by creating a simple function in Python
- The task is to draws a certain number of random variables and return their average
- Imaginatively, we will call the function `average_py()`

]
.panel[.panel-name[Python Code]

```{python}
import random
from time import process_time
def average_py(n):
  s = 0
  for i in range(n):
      s +=random.random()
  return s/n
```
]

.panel[.panel-name[Output]
```{python}
t_start=process_time()
n=1000000
average_py(n)
t_end=process_time()
print("Elapsed time in seconds:",t_end-t_start)
```

]
]

## Loops using `Numpy` {.top}
.panelset[
.panel[.panel-name[Explanation]
- The strength of `Numpy` lies in its vectorisation capabilities.
- Formally, loops vanish on the Python level
- The looping takes place one level deeper based on optimising and compiling routines provided by `Numpy`
- The function `average_np()` makes use of these

]
.panel[.panel-name[Python Code]

```{python}
import numpy as np
def average_np(n):
  s = np.random.random(n)
  return s.mean()
```
]

.panel[.panel-name[Output]
```{python}
t_start=process_time()
n=1000000
average_np(n)
t_end=process_time()
print("Elapsed time in seconds:",t_end-t_start)
```

]
.panel[.panel-name[Vectorisation and Memory]
- The speedup in vectorisation is considerable, almost a factor of 5.
- However this comes at a price of significantly higher memory usage
- The reason is that `Numpy` attains speed by preallocating data that can be processed in the compiled layer.

]
]

## Monte Carlo Simulation in Finance {.top}
- As noted earlier, Monte Carlo simulation is an indepensible numerical tool on quantitative finance.
- Banks and other financial institutions using it for pricing and risk management purposes. 
- It is one of the most powerful, flexible, but computationally demanding methods in quantitative finance. 
- This demand has only increased since Basel III, as banks and financial institutions are required to pricing and hedge counterparty risk of complex derivatives.

## Monte Carlo Simulation of Stock Prices {.top}
.panelset[
.panel[.panel-name[Explanation]
- In derivative pricing it is standard practice to simulate a set of discrete time series paths for the value of a stock
- We can create a hybrid MC simulator by implementing a Python loop on `ndarray` objects.
- As before we will then use this function, mcs_simulation_py(), to benchmark execution time across a number of performance enhancing approaches.
- Specify, this function simulates geometric Brownian motion price paths using Equation 10-2. in Hilpisch (2018) "Python for Finance".
]
.panel[.panel-name[Python Code for MC simulator]

```{python}
import math
S0=36.
T=1.0
r=0.06
sigma=0.2
def mcs_simulation_py(p):
  M,I = p # M= number of time intervals for discretisation. I = number of paths to simulate. 
  dt = T/M
  S = np.zeros((M+1,I))
  S[0]=S0
  rn = np.random.standard_normal(S.shape)
  for t in range(1,M+1):
    for i in range(I):
      S[t,i] = S[t-1,i]*math.exp((r-sigma**2/2)*dt+sigma*math.sqrt(dt) *rn[t,i])
  return S
```
]

.panel[.panel-name[Execution for MC simulator]

```{python}
t_start=process_time()
I=1000
M=100
S = mcs_simulation_py((M,I));
t_end=process_time()
```
```{python}
print("Elapsed time in seconds:",t_end-t_start)
```

]
.panel[.panel-name[Plot 1000 Price Paths in R]
.pull-left[
- `reticulate` allows you to pull objects back into `R` using the `py$OBJECT_NAME` convention.
```
py$S %>% 
  as_tibble() %>%
  mutate(t=row_number()) %>%
  gather(Path,Price,-t) %>%
  ggplot(aes(x=t,y=Price,colour=Path)) + 
  geom_line() +
  theme(legend.position = "none") +
  labs(y="Simulated Prices")
```
]
.pull-right[
```{r}
#| echo: false
#| fig-retina: 3
#| fig-height: 6
py$S %>% 
  as_tibble() %>%
  mutate(t=row_number()) %>%
  gather(Path,Price,-t) %>%
  ggplot(aes(x=t,y=Price,colour=Path)) + 
  geom_line() +
  theme(legend.position = "none") +
  labs(y="Simulated Prices")
```
]
]
]

## MC simulator using `Numpy` {.middle}
.panelset[
.panel[.panel-name[`Numpy` Version]
- The below python code is very similar to the previous except for the highlighted line

```{python}
def mcs_simulation_np(p):
  M,I = p
  dt = T/M
  S = np.zeros((M+1,I))
  S[0]=S0
  rn = np.random.standard_normal(S.shape)
  for t in range(1,M+1):
    for i in range(I):
      S[t,i] = S[t-1,i] * np.exp((r-sigma**2/2) * dt + sigma * math.sqrt(dt) * rn[t,i]) 
  return S
```
]
.panel[.panel-name[Execution time for MC simulator]

```{python}
t_start=process_time()
S = mcs_simulation_np((M,I))
t_end=process_time()
```

```{python}
print("Elapsed time in seconds:",t_end-t_start)
```

- In this instance this is not faster than the base Python code.

]
]

## MC simulator using `Numba` {.top}
.panelset[
.panel[.panel-name[numba simulation]

```{python}
import numba
mcs_simulation_nb=numba.jit(mcs_simulation_np)
```
]
.panel[.panel-name[first run]
### This call includes the compile time overhead

```{python}
t_start=process_time()
S=mcs_simulation_nb((M,I))
t_end=process_time()
print("Elapsed time in seconds:",t_end-t_start)
```
]
.panel[.panel-name[second run]
### This call excludes the compile time overhead

```{python}
from time import process_time
t_start=process_time()
S=mcs_simulation_nb((M,I))
t_end=process_time()
print("Elapsed time in seconds:",t_end-t_start)
```

- After compilation `Numba` is much faster than `Numpy`
]
]

## Multiprocessing {.middle}

.panelset[
.panel[.panel-name[Explanation]

- As mentioned before, Monte Carlo simulation is a task that lends itself well to parallelisation.
- It is an *embarrassingly* parallel problem with no information shared across sub Jobs.
- In the example we could parallelise the simulation of 1000 paths into 10 processes simulating 100 paths each.
- .saltinline[On the server students are restricted to one vCPU, so what follows is better run on a local multiprocessor machine]
]
.panel[.panel-name[Parallelisation in Python]

- .saltinline[The following code makes use of the `multiprocessing` module and divides the total number of paths to be simulated into small chunks of size $I/p$, where $p>0$]

```{python}
#| eval: false
import multiprocessing as mp
pool = mp.Pool(processes=4)
p=10
```

- .saltinline[After all the single tasks are finished, the results are put together in a single `ndarray` object using `np.stack`]

```{python}
#| eval: false
t_start=process_time()
S= np.hstack(pool.map(mcs_simulation_np,p*[(M,int(I/p))]))
t_end=process_time()
print("Elapsed time in seconds:",t_end-t_start)
```

- .heatinline[In this instance there is no more speed-up to be observed.]
]
] 